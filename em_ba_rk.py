# -*- coding: utf-8 -*-
"""EM-BA-RK

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1G5WHzVRnzeWl9RQEGZ1evJ1Bm9PAzMkB
"""

import pandas as pd
output = '/content/Conversation_Dataset.csv'
df = pd.read_csv(output, on_bad_lines='skip', encoding='utf-8')
df.head()

df.isnull().sum()

df.info()
df.describe()

import pandas as pd
import nltk
from nltk.sentiment.vader import SentimentIntensityAnalyzer

# Download the vader_lexicon resource - Handle the case where it's already downloaded
try:
    nltk.data.find('sentiment/vader_lexicon.zip')
except LookupError:
    nltk.download('vader_lexicon')

sia = SentimentIntensityAnalyzer()

# Load data
output = '/content/Conversation_Dataset.csv'
df = pd.read_csv(output, encoding='utf-8')

# Vectorized function to calculate sentiment scores
df['sentiment'] = df['text'].apply(lambda x: sia.polarity_scores(str(x))['compound'])

# Save results
df.to_csv('/content/Conversation_Dataset_with_sentiment.csv', index=False)

import matplotlib.pyplot as plt

# Plot the distribution of sentiment scores
plt.hist(df['sentiment'], bins=50, color='skyblue', edgecolor='black')
plt.title('Sentiment Score Distribution')
plt.xlabel('Sentiment Score')
plt.ylabel('Frequency')
plt.show()

# Get basic statistics
print(df['sentiment'].describe())

def categorize_sentiment(score):
    if score > 0.05:
        return 'Positive'
    elif score < -0.05:
        return 'Negative'
    else:
        return 'Neutral'

# Apply categorization
df['sentiment_category'] = df['sentiment'].apply(categorize_sentiment)

# Example of frequency of sentiment categories
print(df['sentiment_category'].value_counts())

# Filter positive sentiment
positive_df = df[df['sentiment'] > 0.05]

# Filter negative sentiment
negative_df = df[df['sentiment'] < -0.05]

# Display a few rows of positive and negative sentiment
print(positive_df.head())
print(negative_df.head())

print(df.dtypes)

df['date_time'] = pd.to_datetime(df['date_time'], errors='coerce')

print(df.dtypes)

print(df['date_time'].isna().sum())  # Count NaT values
df.dropna(subset=['date_time'], inplace=True)

df['response_time'] = df.groupby('conversation_id')['date_time'].diff().dt.total_seconds()
df['response_time'] = df['response_time'].fillna(0)  # Fixed warning

print(df[['date_time', 'response_time']].head(10))

df['is_query_resolved'] = df['text'].str.contains(
    "resolved|fixed|thank you|issue solved|problem solved",
    case=False,
    na=False
).astype(int)

df['conversation_length'] = df.groupby('conversation_id')['text'].transform('count')

features = df[['sentiment', 'response_time', 'is_query_resolved', 'conversation_length']].fillna(0)

from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
features_scaled = scaler.fit_transform(features)

from sklearn.cluster import KMeans

kmeans = KMeans(n_clusters=3, random_state=42)
df['cluster'] = kmeans.fit_predict(features_scaled)

cluster_map = {
    df.groupby('cluster')['sentiment'].mean().idxmax(): "Satisfied",
    df.groupby('cluster')['sentiment'].mean().idxmin(): "Not Satisfied",
    df.groupby('cluster')['sentiment'].mean().idxmax(skipna=True): "Neutral"
}

df['satisfaction'] = df['cluster'].map(cluster_map)

import seaborn as sns
import matplotlib.pyplot as plt

sns.countplot(x=df['satisfaction'])
plt.title("Distribution of Predicted Satisfaction Levels")
plt.show()

sns.scatterplot(x=df['sentiment'], y=df['response_time'], hue=df['satisfaction'])
plt.title("Sentiment vs Response Time Clustering")
plt.show()

df.to_csv('customer_satisfaction_predictions.csv', index=False)

# Categorize sentiment into Positive, Neutral, Negative
def categorize_sentiment(score):
    if score > 0.05:
        return 'Positive'
    elif score < -0.05:
        return 'Negative'
    else:
        return 'Neutral'

df['sentiment_category'] = df['sentiment'].apply(categorize_sentiment)

# Group by date and calculate sentiment distribution
sentiment_distribution_by_date = df.groupby([df['date_time'].dt.date, 'sentiment_category']).size().unstack(fill_value=0)

# Plot sentiment distribution by date
sentiment_distribution_by_date.plot(kind='line', figsize=(10, 6))
plt.title('Sentiment Distribution by Date')
plt.xlabel('Date')
plt.ylabel('Frequency')
plt.xticks(rotation=45)
plt.show()

import pandas as pd
df = pd.read_csv('/content/Conversation_Dataset_with_sentiment.csv')

"""DBSCAN"""

df.head()

import matplotlib.pyplot as plt
import seaborn as sns
plt.figure(figsize=(8,5))
sns.histplot(df['sentiment'], bins=50, kde=True, color='blue')
plt.title("Sentiment Score Distribution")
plt.xlabel("Sentiment Score")
plt.ylabel("Count")
plt.show()

df_grouped = df.groupby('conversation_id')['sentiment'].mean().reset_index()

plt.figure(figsize=(8,5))
sns.histplot(df_grouped['sentiment'], bins=30, kde=True, color='purple')
plt.title("Average Sentiment Per Conversation")
plt.xlabel("Mean Sentiment Score")
plt.ylabel("Count of Conversations")
plt.show()

# Import necessary libraries
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from wordcloud import WordCloud

# Select first 10,000 customer messages
customer_text = ' '.join(df[df['speaker'] == 'Customer']['text'].dropna().head(10000))

# Instantiate wordcloud
wordcloud = WordCloud(width=800, height=400, background_color='white').generate(customer_text)

# Plot the Word Cloud
plt.figure(figsize=(10,5))
plt.imshow(wordcloud, interpolation="bilinear")
plt.axis("off")
plt.title("Most Common Words in First 10,000 Customer Messages")
plt.show()

import pandas as pd
df = pd.read_csv('/content/Conversation_Dataset_with_sentiment.csv')

import random
import matplotlib.pyplot as plt

# Sample up to 10 unique conversation IDs
conversation_ids = df['conversation_id'].unique()
num_samples = min(10, len(conversation_ids))
sampled_conversation_ids = random.sample(list(conversation_ids), num_samples)

# Filter DataFrame for only sampled conversations
df_sample = df[df['conversation_id'].isin(sampled_conversation_ids)].copy()

# Add 'message_number' to track message order
df_sample['message_number'] = df_sample.groupby('conversation_id').cumcount() + 1

# Plot sentiment flow
plt.figure(figsize=(10, 5))

for convo_id, data in df_sample.groupby('conversation_id'):
    plt.plot(data['message_number'], data['sentiment'], alpha=0.5, label=convo_id)

plt.xlabel("Message Number in Conversation")
plt.ylabel("Sentiment Score")
plt.title("Sentiment Flow (Sampled Conversations)")
plt.legend(loc="upper right", fontsize="small", ncol=2)  # Show legend
plt.show()

import seaborn as sns
plt.figure(figsize=(8,5))
sns.histplot(df.groupby('conversation_id')['text'].count(), bins=50, color='purple')
plt.title("Conversation Length Distribution")
plt.xlabel("Number of Messages per Conversation")
plt.ylabel("Count")
plt.show()